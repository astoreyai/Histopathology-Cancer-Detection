{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from scripts.data_utils import HistopathologyDataModule\n",
    "from scripts.model_utils import BaselineCNN\n",
    "from scripts.config import BATCH_SIZE, LEARNING_RATE, EPOCHS, TARGET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Setup MLflow for logging\n",
    "mlflow_logger = MLFlowLogger(\n",
    "    experiment_name=\"Histopathology Cancer Detection - Training\",\n",
    "    tracking_uri=\"file:./experiments/mlruns\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the function for training the model\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Trains the histopathology CNN model with PyTorch Lightning.\n",
    "    Handles logging, checkpoints, and validation during training.\n",
    "    \"\"\"\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    # Setup MLflow Logger\n",
    "    mlflow_logger = MLFlowLogger(\n",
    "        experiment_name=\"Histopathology Cancer Detection - Training\",\n",
    "        tracking_uri=\"file:./experiments/mlruns\"\n",
    "    )\n",
    "\n",
    "    # Initialize DataModule\n",
    "    data_module = HistopathologyDataModule(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        target_size=TARGET_SIZE\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = BaselineCNN(input_shape=(3, *TARGET_SIZE), learning_rate=LEARNING_RATE)\n",
    "\n",
    "    # Define callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=\"checkpoints/\",\n",
    "        filename=\"best_model-{epoch:02d}-{val_loss:.4f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "    # Initialize trainer (removing parallelization)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        logger=mlflow_logger,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",  # Use GPU if available\n",
    "        devices=1,  # Only one device (CPU or GPU)\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "        log_every_n_steps=50,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    # Save the best model\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "    # Log the best model path to MLflow\n",
    "    mlflow_logger.experiment.log_param(\n",
    "        run_id=mlflow_logger.run_id,\n",
    "        key=\"best_model_path\",\n",
    "        value=best_model_path\n",
    "    )\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Execute the training if running as a standalone notebook\n",
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
